# -*- coding: utf-8 -*-
"""churn_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DZ7feD_qGVsW-dImtETIMP9LzwVHrqkw

File Upload and check it's work or not
"""

import pandas as pd

df = pd.read_csv('/content/converted_file.csv')  # Changed to pd.read_csv

# Display first 5 rows
df.head()

"""Check The Null values

"""

# Check basic information
df.info()

# Check missing values
df.isnull().sum()

# Check summary statistics
df.describe()

"""Churn resons a null so we remove it

Reson :

1. High Amount of missing values
2. To fill that values create a new model and high data-set needed
3. And, also in this project only consider month,monthly charges.....
"""

import pandas as pd

# Load the CSV file
file_path = "/content/converted_file.csv"
df = pd.read_csv(file_path)

# Check for missing values
print(df.isnull().sum())

df = df.drop(columns=['CustomerID', 'Zip Code', 'Lat Long', 'Latitude', 'Longitude', 'Churn Reason'])

# Check the dataset again
df.head()

df = pd.get_dummies(df, drop_first=True)

# Check the dataset after encoding
df.head()

from sklearn.model_selection import train_test_split

# Define X (features) and y (target)
X = df.drop(columns=['Churn Value'])  # Drop target column from features
y = df['Churn Value']  # Target column

# Split data into 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape  # Check sizes of train & test sets

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Define features (X) and target (y)
X = df.drop(columns=['Churn Value'])  # Features
y = df['Churn Value']  # Target (0 = No Churn, 1 = Churn)

# Split into training (80%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data (for models like Logistic Regression)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train Logistic Regression model
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)

# Make predictions
y_pred_lr = lr_model.predict(X_test)

# Evaluate the model
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))
print("Classification Report:\n", classification_report(y_test, y_pred_lr))

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

import matplotlib.pyplot as plt
import pandas as pd

# Load the dataset
file_path = "/content/converted_file.csv"
df = pd.read_csv(file_path)

# Remove non-numeric columns for feature importance analysis
df_numeric = df.select_dtypes(include=['number'])

# Define features (X) and target (y)
X = df_numeric.drop(columns=['Churn Value'])  # Features
y = df_numeric['Churn Value']  # Target (0 = No Churn, 1 = Churn)

# Re-train Random Forest Model for Feature Importance
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Get Feature Importance
feature_importance = rf_model.feature_importances_
features_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})
features_df = features_df.sort_values(by='Importance', ascending=False)

# Plot Feature Importance (Top 10 Features)
plt.figure(figsize=(10, 6))
plt.barh(features_df['Feature'][:10], features_df['Importance'][:10], color='royalblue')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.title('Top 10 Important Features for Churn Prediction')
plt.gca().invert_yaxis()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("/content/converted_file.csv")

# Ensure 'Churn Label' and 'Contract' are treated as categorical
df['Churn Label'] = df['Churn Label'].astype(str)
df['Contract'] = df['Contract'].astype(str)

# Churn by Contract Type (Pie Chart)
plt.figure(figsize=(6, 6))
df['Contract'].value_counts().plot.pie(autopct='%1.1f%%', colors=['skyblue', 'lightcoral', 'lightgreen'])
plt.title("Subscription Cancellation by Contract Type")
plt.ylabel("")  # Hide y-axis label
plt.show()

from google.colab import drive
drive.mount('/content/drive')

# @title Default title text
tenure_churn = df.groupby(['Tenure Months', 'Churn Label']).size().unstack()

tenure_churn.plot(kind='bar', stacked=True, figsize=(10, 5), color=["green", "red"])
plt.title("Churn vs. No Churn by Tenure")
plt.xlabel("Tenure (Months)")
plt.ylabel("Number of Customers")
plt.legend(title="Churn Label", labels=["No Churn", "Churn"])
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""### Insights from the Graphs:

1️⃣ **Monthly Charges and Customer Dropouts**  
   - Customers paying **higher monthly charges** are more likely to stop using the service.  
   - Those who stay usually have **lower monthly charges**.  
   - This means **expensive plans may lead to more dropouts**.  

2️⃣ **Time Spent as a Customer (Tenure in Months)**  
   - Most people who stop using the service leave **within the first few months**.  
   - The longer customers stay, the less likely they are to leave.  
   - Many long-term customers (70+ months) **continue using the service**.  

### Key Takeaways:  
✅ **Lower prices or discounts** may help keep customers.  
✅ **Better onboarding support** for new customers can reduce early dropouts.  
✅ **Loyalty programs** can encourage customers to stay longer.  

# **our analysis helps understand why customers stop using a service by identifying patterns in factors like:**


✅ Monthly Charges – Higher costs may lead to more dropouts.

✅ Tenure – New customers leave more often, while long-term customers stay.

✅ Contract Type – Month-to-month users leave more compared to yearly contracts.

✅ Feature Importance – Certain factors have a bigger impact on whether a customer stays or leaves.
"""